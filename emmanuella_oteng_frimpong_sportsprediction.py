# -*- coding: utf-8 -*-
"""Emmanuella Oteng Frimpong_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BoVYn8pVaMM_mkJisUNagsXnRElyfXg5

INTRODUCTION

** Author: Emmanuella Oteng Frimpong

** Assignment 2

#Description

In sports prediction, large numbers of factors including the historical performance of the teams, results of matches, and data on players, have to be accounted for to help different stakeholders understand the odds of winning or losing.

The tasks that are supoosed to be performed are;

Demonstrate the data preparation & feature extraction process
Create feature subsets that show maximum correlation with the dependent variable.
Create and train a suitable machine learning model with cross-validation that can predict a player's rating.
Measure the model's performance and fine-tune it as a process of optimization.
Use the data from another season(players_22) which was not used during the training to test how good is the model.
Deploy the model on a simple web page using either (Heroku, Streamlite, or Flask) and upload a video that shows how the model performs on the web page/site.
"""

from google.colab import drive
drive.mount('/content/drive')

"""##Imports and Loading

This section of the notebook will be used for installing and loading datasets and libraries.

"""

!pip install pandas numpy matplotlip seaborn xgboost scikit-learn

!python --version

np.__version__

## Importing necessary llibraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import pickle

from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, VotingRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

"""After importing the needed libraries, now we have to load datasets."""

##loading the datasets.
male_legacy_df = pd.read_csv("/content/drive/MyDrive/INTRO TO AI/male_players (legacy).csv") ## for training the model
player22_df = pd.read_csv("/content/drive/MyDrive/INTRO TO AI/players_22-1.csv") ## for testing the model

"""# Data Preprocessing

## EDA, Imputation and Encoding.

In this section, since we know we have already loaded our data therefore we would be performing an exploratory data analysis, identifying features that are important to us, doing imputation and performing encoding on all the necessary columns.

This step is necessary for the transformation of our data since not all columns, rows are needed for the analysis.
"""

#view first few rows and the nature of the data
male_legacy_df.head()

player22_df.head()

#try to understand the nature oof the data.
male_legacy_df.info()

player22_df.info()

"""From the infomation decribed above, we can see that there are a lot of columns to consider for the analysis. So we are going to need to drop some, before finding the needed features we can work with, let's get the number of missing values for each of our dataframes"""

#Checking for missing data
print("Checking sum of missing value for Players 21(Train Data:)")
male_legacy_df.isnull().sum()

print("Checking sum of missing value for Players 22(Test Data:)")
player22_df.isnull().sum()

"""## Dropping missing values

Now that we know the information and the columns with lots of missing values, we are going to drop the ones that have 30% or more of the data missing.
"""

total_rows_male = male_legacy_df.shape[0] #shape for train data
total_rows_player22 = player22_df.shape[0] #shape for test data

"""Calculate the 30% threshhold for the two sets"""

threshold_male = int(0.3 * total_rows_male)
threshold_player22 = int(0.3 * total_rows_player22)

print("The threshold for Males Legacy is", threshold_male)

print("The theshold for Players 22 is", threshold_player22)

"""Get a list of all columns with a sum of missing values greater than the threshold:"""

columns_to_drop = []
for column in male_legacy_df.columns:
    if male_legacy_df[column].isna().sum() > threshold_male:
        columns_to_drop.append(column)

columns_to_drop_22 = []
for column in player22_df.columns:
    if  player22_df[column].isna().sum() > threshold_player22:
        columns_to_drop_22.append(column)

"""Drop the columns:"""

male_legacy_df = male_legacy_df.drop(columns=columns_to_drop, axis=0)
player22_df = player22_df.drop(columns=columns_to_drop_22 , axis=0)

"""So now leets check the info again to see if the columns dropped."""

#understand nature of data
male_legacy_df.info()

player22_df.info()

"""After reviewing kaggle, reading the data description and looking at things using the data explorer, I came to understand some comuns don't contribute to the overall rating of a player, so we are going to drop those columns too.

Here is a link to exploer the columns in the data: [Data Explorer on Kaggle for Players 22](https://www.kaggle.com/datasets/stefanoleone992/fifa-22-complete-player-dataset/?select=players_22.csv)

and [Data Explorer on Kaggle for Males Legacy](https://www.kaggle.com/datasets/stefanoleone992/fifa-23-complete-player-dataset?select=male_players+%28legacy%29.csv)

For males legacy csv, lets drop those columns.
"""

drop_columns = ['club_contract_valid_until_year','club_name','club_position','club_joined_date','international_reputation','nationality_id','club_team_id','player_id','player_url','fifa_version','long_name','dob','body_type','real_face','player_face_url', 'fifa_update', 'fifa_update_date']

male_legacy_df = male_legacy_df.drop(drop_columns, axis=1)

drop_columns = ['club_contract_valid_until','club_name','club_position','international_reputation','nationality_id','club_team_id','sofifa_id','player_url','long_name','dob','body_type','real_face','player_face_url','club_logo_url','club_flag_url','nation_flag_url']

player22_df = player22_df.drop(drop_columns, axis=1)

"""After a further review, some columns were identified that could be dropped with this justfication. If we look at the `ls` column it is described as the `player attribute playing as LW`.

Such columns are only useful if we wanted to predict a player's effectiveness in playing such a position, so we drop such columns with that description.


Players are normally played in a specific posiion at their clubs which contibutes more to their overall rating, thus columns like `players_positions` which is the `player preferred positions`


Other columns reviewd that can be dropped are;
*   `short_name`
*   `club_joined`
*   `nationality_name`
"""

#drop new identified columnas
drop_r_cols = ['short_name', 'player_positions', 'league_name', 'nationality_name', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']

male_legacy_df  = male_legacy_df .drop(drop_r_cols, axis=1)
player22_df = player22_df.drop(drop_r_cols, axis=1)

"""## Imputation x Encoding

Having dropped columns with many missing values, we can noe begin our imutation. Imputation is where we will fill missing data with certain values.
"""

## Filling missing numeric data with the mean value
num_imputer = SimpleImputer(strategy='mean')

## Filling missing categorical data with the most frequent value
cat_imputer = SimpleImputer(strategy='most_frequent')

# Selecting numerical and categorical features

num_features_males = male_legacy_df.select_dtypes(include=[np.number]).columns.tolist()
cat_features_males = male_legacy_df.select_dtypes(include=[object]).columns.tolist()

num_features_22 = player22_df.select_dtypes(include=[np.number]).columns.tolist()
cat_features_22 = player22_df.select_dtypes(include=[object]).columns.tolist()

cat_features_males, cat_features_22

"""From Kaggle the column `overall` is described as the *player current overall attribute* which transalte to the **the player rating** i.e the crux of this whole project. Thus we remove `overall` since it is our target variable."""

# Removing the target variable from the features
num_features_males.remove('overall')  # 'overall' is the target variable
num_features_22.remove('overall')  # 'overall' is the target variable

"""Now to the Imputation;

"""

#males legacy imputation
male_legacy_df[num_features_males] = num_imputer.fit_transform(male_legacy_df[num_features_males])

#categorical imputation
male_legacy_df[cat_features_males] = cat_imputer.fit_transform(male_legacy_df[cat_features_males])

#player 22 imputation
player22_df[num_features_22] = num_imputer.fit_transform(player22_df[num_features_22])

#categorical imputation
player22_df[cat_features_22] = cat_imputer.fit_transform(player22_df[cat_features_22])

male_legacy_df.shape

player22_df.shape

"""Next task is to perform is the encoding. We do this for only categorical columns. We first explored encoding use OneHot Encoding technique, but quickly discovered that we run out of memory so quickly pivoted to encoding using pd.get_dummies"""

# Using `get_dummies` for one-hot encoding and dropping the first category
male_legacy_encoded_df = pd.get_dummies(male_legacy_df, columns=cat_features_males, drop_first=True)
player22_encoded_df = pd.get_dummies(player22_df, columns=cat_features_22, drop_first=True)

male_legacy_encoded_df.head()  # display the first few rows to verify the changes

male_legacy_encoded_df.shape

"""Finally, we can describe our datasets before we start the feature analysis."""

male_legacy_encoded_df.describe

player22_encoded_df.describe

"""#Feature Engineering.

## Feature Extraction
Now we are going to analyze the dataset to understand which features are important for determining a player's overall rating. We are using feature importance *to* identify necessary features.


Here we are fitting a RandomForestRegressor to obtain feature importances.
"""

# the target variable and features; drop non-numeric columns if necessary
X = male_legacy_encoded_df.drop(columns=['overall'])
y = male_legacy_encoded_df['overall']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)

# Create a Random Forest Regressor model
model = RandomForestRegressor(random_state=69)
model.fit(X_train, y_train)

# Get feature importances
importances = model.feature_importances_

# Sort them in descending order
indices = np.argsort(importances)[::-1]

# Let's print out the feature importance ranking
print("Top 20 Feature ranking:")

for i in range(21):
    print(f"{i + 1}. Feature {X.columns[indices[i]]} ({importances[indices[i]]})")

#Now, let's get the top 10 features
top_features = [X.columns[indices[i]] for i in range(10)]
print("\nTop 10 features with % Contribution:")

for i in range(10):
    print(f"{i + 1}.  {top_features[i]} ({round(importances[indices[i]]*100,2)}%)")

"""Observing the results of the feature importance process, it can be seen that the top 5 features contribute a percentage importance of *97%*.

Thus my strategy is to use the top 10 features to train so I capture the underlying data patterns even for weak contributing features. Then when testing use the same 5. And, when deployed in the future use the top 5 features for prediction.

Let's see how it Goes. On to Feature subsetting.
"""

top_features = top_features[:5]

print('Features being used for model development are:\n')
top_features

"""## Feature Subset

At this stage our goal is to use the top features we have identified during our feature extraction stage to create subsetted data that we will use to train models.
"""

#Now we subset our X feauture set
X_top_f = X[top_features]
X_top_f

#no need to do for y

"""Now lets scale our features which is our independent variables"""

# Initialize the scaler
scaler = StandardScaler()

# Scale the features
X_scaled = scaler.fit_transform(X_top_f)

# The features are now scaled and ready for training the model.
X_scaled_df = pd.DataFrame(X_scaled, columns=X_top_f.columns)

X_scaled_df.head()

#Saving scaler to use in deployment

with open('/content/drive/MyDrive/INTRO TO AI/scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

"""# Training Models

We are now reading to train some models, here we are going to train 3 modes;
1. XGBoost
2. Gradient Boost
3. Random Forest


Lets split data for training
"""

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=84)

"""Now we define a function for our different training models.

"""

def train_model(model, param_grid, X, y):
    '''
        Trains a model using grid search with cross-validation and returns the best model.
        Parameters:
            model: scikit-learn model
            param_grid: dictionary with parameters to try
            X: features(independent variables)
            y: target(dependent variable)
    '''
    cv = KFold(n_splits=7 , random_state=69, shuffle=True)

    # Grid search with cross-validation
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)
    grid_search.fit(X, y)

    # Results of the grid search
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best score (MAE): {-grid_search.best_score_}")  # We negate the score because grid search maximizes performance (so it negates the scores)

    return grid_search.best_estimator_  # Returns the best model

"""## Model 1: XGBoost

"""

print("\nTraining XGBoost...")
xgb_model = xgb.XGBRegressor(random_state=42)
xgb_params = {
    'n_estimators': [10, 50, 100],
    'learning_rate': [0.1, 0.01],
    'max_depth': [3, 5, 15],
    'colsample_bytree': [0.5,1]
}
best_xgb = train_model(xgb_model, xgb_params, X_train, y_train)

"""##Model 2: Gradient Boosting Regressor"""

print("\nTraining Gradient Boosting...")
gbr_model = GradientBoostingRegressor(random_state=63)
gbr_params = {
    'n_estimators': [10, 50,  100,],
    'learning_rate': [0.1, 0.01],
    'max_depth': [9, 15]
}
best_gbr = train_model(gbr_model, gbr_params, X_train, y_train)

"""## Model 3: Random Forest

"""

print("\nTraining Random Forest...")
rf_model = RandomForestRegressor(random_state=39)
rf_params = {
    'n_estimators': [10, 50, 100],
    'max_depth': [12, 15],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}
best_rf = train_model(rf_model, rf_params, X_train, y_train)

"""## Model 4: Ensembled Model
Form discussion in class I have come to understand that, an ensembled model can improve a model's predicitve perfromance. Here I will combine the best versions of my 3 models into a single ensemble model.
"""

# Create an ensemble model
ensemble = VotingRegressor(
    estimators=[
        ('xgb', best_xgb),
        ('gbr', best_gbr),
        ('rf', best_rf)
    ]
)

# Fit model on the training data
print("\nTraining Ensemble Model...")
ensemble.fit(X_train, y_train)

# Predict and evaluate on the training set
train_pred = ensemble.predict(X_train)
train_mae = mean_absolute_error(y_train, train_pred)
print(f"Ensemble model MAE on training set: {train_mae}")

"""### Loading Saved Models"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/Intro to ai'

with open('best_xgb_model.pkl', 'wb') as file:
    pickle.dump(best_xgb, file)

with open('best_gbr_model.pkl', 'wb') as file:
    pickle.dump(best_gbr, file)

with open('best_rf_model.pkl', 'wb') as file:
    pickle.dump(best_rf, file)

with open('ensemble_model.pkl', 'wb') as file:
    pickle.dump(ensemble, file)

with open('ensemble_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

predictions = loaded_model.predict(X_test)

en_mae = mean_absolute_error(y_test, predictions)

print(f"Ensemble model MAE on test set: {en_mae}")

"""### Testing"""

print("\nEvaluating XGBoost...")

#predict on test set
pred_xgb = best_xgb.predict(X_test)
xgb_mae = mean_absolute_error(y_test, pred_xgb)

print(f"XGBoost model MAE on test set: {xgb_mae:.2f}")

print("\nEvaluating Gradient Boost...")

#predict on test set
pred_gbr = best_gbr.predict(X_test)
gbr_mae = mean_absolute_error(y_test, pred_gbr)

print(f"Gradient Boost Regressor model MAE on test set: {gbr_mae}:.2f")

print("\nEvaluating Ensemble...")

#predict on test set
pred_en = ensemble.predict(X_test)
en_mae = mean_absolute_error(y_test, pred_en)

print(f"Ensemble model MAE on test set: {en_mae:.2f}")

!pip freeze > requirements.txt